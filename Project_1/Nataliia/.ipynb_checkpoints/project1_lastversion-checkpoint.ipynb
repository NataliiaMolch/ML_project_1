{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vj1NmipsPbQH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "def normalize_data(tx):\n",
    "    # normalizing data by features and add bias\n",
    "    input_data = np.zeros_like(tx, dtype = np.float)\n",
    "    for i in range(input_data.shape[1]):\n",
    "      input_data[:,i] = tx[:,i] -  np.mean(tx[:,i])\n",
    "      input_data[:,i] = tx[:,i]/np.std(tx[:,i])\n",
    "    return  np.concatenate((np.ones((tx.shape[0],1)), input_data), axis = 1)\n",
    "def check_linear_dependenece(tx):\n",
    "    #check if there are lineary dependent cols\n",
    "    print(\"Feature matrix rank is \", np.linalg.matrix_rank(matrix, tol = 1e-5))\n",
    "    for i in range(matrix.shape[1]):\n",
    "      for j in range(matrix.shape[1]):\n",
    "        if i != j:\n",
    "            inner_product = np.inner(\n",
    "                matrix[:,i],\n",
    "                matrix[:,j]\n",
    "            )\n",
    "            norm_i = np.linalg.norm(matrix[:,i])\n",
    "            norm_j = np.linalg.norm(matrix[:,j])\n",
    "\n",
    "            if np.abs(inner_product - norm_j * norm_i) < 1E-5:\n",
    "                print( 'Dependent rows #{} and #{}'.format(i, j) )\n",
    "                print( 'I: ', matrix[:,i])\n",
    "                print('J: ', matrix[:,j])\n",
    "                print( 'Prod: ', inner_product)\n",
    "                print('Norm i: ', norm_i)\n",
    "                print('Norm j: ', norm_j)\n",
    "def remove_outliers(X, y, threshold=5.0):\n",
    "    idx = (np.abs(X) < threshold).all(axis=1)\n",
    "    return X[idx], y[idx], idx\n",
    "def replace_missing_values(x):\n",
    "  nr = x.shape[0]\n",
    "  mean = [np.mean(x[:,i]) for i in range(x.shape[1])]\n",
    "  for i in range(nr):\n",
    "    for ind in np.where(x[i] == -999)[0]:\n",
    "        x[i,ind] = mean[ind]\n",
    "  return x\n",
    "def delete_missing_values(x, y):\n",
    "  row_nmb = []\n",
    "  nr = x.shape[0]\n",
    "  for i in range(nr):\n",
    "    if np.sum(x[i] == -999.) > 0:\n",
    "        row_nmb.append(i)\n",
    "  print(\"% of axis to be deleted is \", len(row_nmb)/nr)\n",
    "  return np.delete(x, row_nmb, axis = 0), np.delete(y, row_nmb, axis = 0)\n",
    "  \"\"\"\n",
    "  dist = compute_pairwise_dist_of_rows(x)\n",
    "  print(\"Searching for missing values\")\n",
    "  nr, nc = x.shape\n",
    "  for i in range(nr):\n",
    "    if (x[i] == -999).shape[0] > 0:\n",
    "      where = np.where(i == -999)\n",
    "      for j in where:\n",
    "        x[i,j] = x[np.where(dist == np.amin(dist[i]))[0], k ] + np.random.randn()*np.std(x[:,k])/2.\n",
    "  return normalize_data(x)\n",
    "    \"\"\"\n",
    "def MSE(y, tx, w):\n",
    "    return np.average((y - np.dot(tx,w)) ** 2)\n",
    "def MSE(y, tx, w):\n",
    "    return np.average((y - np.dot(tx,w)) ** 2)\n",
    "def MSE_long(y, tx, w): #is made because usual computation causes RAM overflow\n",
    "    mse = 0.\n",
    "    len_y = len(y)\n",
    "    for i in range(len_y):\n",
    "      mse += (y[i] - np.dot(tx[i], w)) **2\n",
    "    return mse/np.float(len_y)\n",
    "def MAE(y,tx, w):\n",
    "    return np.average(np.abs(y - np.dot(tx,w)))\n",
    "def RMSE(y,tx,w):\n",
    "    return np.sqrt(2.*MSE_long(y,tx,w))\n",
    "def loss_function(y,tx,w):\n",
    "    return MSE_long(y,tx,w)\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    tmp = np.dot(tx,w)\n",
    "    tmpp = y + (tmp<0).astype(np.float) - (tmp>=0).astype(np.float)\n",
    "    return -np.dot(tx.T, tmpp)/float(y.shape[0])\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    tmp = np.dot(tx,w)\n",
    "    tmpp = y + (tmp<0).astype(np.float) - (tmp>=0).astype(np.float)\n",
    "    return np.expand_dims(np.average(-np.dot(tx.T, tmpp), axis = 1), axis =1)\n",
    "def batch_iter(y, tx, batch_size = 10, shuffle=True):\n",
    "    data_size = len(y)\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    num_batches = int(data_size/batch_size)\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "          \n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    r = 0.75\n",
    "    for n_iter in range(max_iters):\n",
    "        gamma = 1./pow(np.float(1+n_iter), r)\n",
    "        w = w - gamma*compute_gradient(y, tx, w)\n",
    "        if gamma <= 1e-5:\n",
    "          break\n",
    "        if n_iter%100 == 0:\n",
    "          print(\"Making {} iteration, {} iterations remain\".format(n_iter, max_iters - n_iter))\n",
    "    print(\"DG returnind results, gamma = {}\".format(gamma))\n",
    "    return w,loss_function(y,tx,w)  \n",
    "    \n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma): #using batch\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    n_iter = 0\n",
    "    r = 0.75\n",
    "    while(n_iter < max_iters):\n",
    "      for b_y,b_tx in batch_iter(y, tx, batch_size = 1, shuffle=True):\n",
    "          gamma = 1./pow(np.float(1+n_iter), r)\n",
    "          w = w - gamma*compute_stoch_gradient(b_y, b_tx, w)\n",
    "          n_iter+=1\n",
    "          if n_iter%10000 == 0:\n",
    "            print( \"Iteration #{}, gamma = {}\".format(n_iter, gamma))\n",
    "          if n_iter >= max_iters or gamma <= 1e-5:\n",
    "              break\n",
    "    \n",
    "    print(\"SDG returnind results, gamma = {}\".format(gamma))\n",
    "    return w,loss_function(y,tx,w)\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    w0 = np.dot(np.linalg.inv(np.dot(tx.T, tx)), np.dot(tx.T, y))\n",
    "    return w0, loss_function(y,tx,w0)\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    N, d = tx.shape\n",
    "    w0 = np.dot(np.linalg.inv(np.dot(tx.T, tx) + np.float(2*N*lambda_)*\n",
    "                              np.ones((d,d), dtype = np.float)), np.dot(tx.T, y))\n",
    "    return w0, loss_function(y,tx,w0)\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma, hessian = True):\n",
    "    def sigmoid(x):\n",
    "        return 1./(1.+ np.exp(-x))\n",
    "    def log_loss(y, tx, w):\n",
    "        \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "        tmp = 0.\n",
    "        print(\"Calculating log loss\")\n",
    "        for i in range(len(y)):\n",
    "            tmp += (np.log(1+ np.exp(tx[i] @ w)) - y[i]*(tx[i] @ w))\n",
    "        return tmp\n",
    "    def calculate_stoch_gradient(y_i, tx_i, w):\n",
    "        return tx_i.T * (sigmoid(np.dot(tx_i, w)) - y_i)\n",
    "    r = 0.75\n",
    "    n_iter = 0\n",
    "    w = initial_w\n",
    "    while(n_iter < max_iters):\n",
    "        for b_y,b_tx in batch_iter(y, tx, batch_size = 1, shuffle=True):\n",
    "            gamma = 1./pow(np.float(1+n_iter), r)\n",
    "            if n_iter%10000 == 0:\n",
    "                print( \"Iteration #{}, gamma = {}\".format(n_iter, gamma))\n",
    "            w -= gamma*calculate_stoch_gradient(b_y, b_tx, w)\n",
    "            n_iter +=1\n",
    "            if n_iter >= max_iters or gamma <= 1e-5:\n",
    "                break\n",
    "        if n_iter >= max_iters or gamma <= 1e-5:\n",
    "            break\n",
    "    return w, log_loss(y,tx,w)\n",
    "def reg_logistic_regression(y, tx, lambda_ , initial_w, max_iters, gamma):\n",
    "    def sigmoid(x):\n",
    "        if x < 11:\n",
    "            return 1./(1.+np.exp(-x))\n",
    "        else:\n",
    "            return 1.\n",
    "    def log_loss(y, tx, w):\n",
    "        \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "        tmp = 0.\n",
    "        print(\"Calculating log loss\")\n",
    "        for i in range(len(y)):\n",
    "            tmpp = tx[i] @ w\n",
    "            if tmpp > 11:\n",
    "                tmp += (1 - y[i])*tmpp\n",
    "            else:\n",
    "                tmp += (np.log(1+ np.exp(tx[i] @ w)) - y[i]*(tx[i] @ w))\n",
    "        return tmp\n",
    "    def calculate_stoch_gradient(y_i, tx_i, w):\n",
    "        return tx_i.T * (sigmoid(np.dot(tx_i, w)) - y_i)\n",
    "    r = 0.75\n",
    "    n_iter = 0\n",
    "    w = initial_w\n",
    "    while(n_iter < max_iters):\n",
    "        for b_y,b_tx in batch_iter(y, tx, batch_size = 1, shuffle=True):\n",
    "            gamma = 1./pow(np.float(1+n_iter), r)\n",
    "            \"\"\"\n",
    "            if n_iter%10000 == 0:\n",
    "                print( \"Iteration #{}, gamma = {}\".format(n_iter, gamma))\n",
    "            \"\"\"\n",
    "            w -= gamma*(calculate_stoch_gradient(b_y, b_tx, w) + lambda_*w)\n",
    "            \n",
    "            n_iter +=1\n",
    "            if n_iter >= max_iters or gamma <= 1e-5:\n",
    "                break\n",
    "        if n_iter >= max_iters or gamma <= 1e-5:\n",
    "            break\n",
    "    return w, log_loss(y,tx,w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZROaA1-gP-dS"
   },
   "outputs": [],
   "source": [
    "\"\"\"some helper functions for project 1.\"\"\"\n",
    "import csv\n",
    "\n",
    "\n",
    "def load_csv_data(data_path, zero_one , sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    if zero_one:\n",
    "        yb[np.where(y=='b')] = 0.\n",
    "    else:\n",
    "        yb[np.where(y=='b')] = -1.\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def predict_labels(weights, data, thh):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred < thh)] = -1.\n",
    "    y_pred[np.where(y_pred >= thh)] = 1.\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "              writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1571572634007,
     "user": {
      "displayName": "JEANS EQUALS ART",
      "photoUrl": "",
      "userId": "06790227118371604310"
     },
     "user_tz": -120
    },
    "id": "llejv-_AO-rH",
    "outputId": "f6f47755-2bf3-4461-df62-bdaf5a1bc637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "path ='../../../data/'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = path + 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, _ = load_csv_data(DATA_TRAIN_PATH, zero_one = True)\n",
    "tX = replace_missing_values(tX)\n",
    "tX = normalize_data(tX)\n",
    "tX,y,_ = remove_outliers(tX, y, threshold=5.0)\n",
    "\n",
    "DATA_TEST_PATH = path + 'test.csv' # TODO: download train data and supply path here \n",
    "y_test, tX_test,_ = load_csv_data(DATA_TEST_PATH, zero_one = True)\n",
    "tX_test = replace_missing_values(tX_test)\n",
    "tX_test = normalize_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:177: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating log loss\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    ac = 0.\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true == y_pred:\n",
    "            ac += 1.\n",
    "    return ac/np.float(len(y_true))\n",
    "lambdas = np.linspace(-5, 0,15)\n",
    "losses = []\n",
    "acc = []\n",
    "N,d = tX.shape\n",
    "\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    w, loss = reg_logistic_regression(y, tX, lambda_, initial_w = 0.0001*np.ones((d,1)), max_iters = 1.1 * N, gamma = 0.7)\n",
    "    y_pred = np.dot(tX_test, w)\n",
    "    y_pred[np.where(y_pred < 0.5)] = 0.\n",
    "    y_pred[np.where(y_pred >= 0.5)] = 1.\n",
    "    acc.append(accuracy(y_test, y_pred))\n",
    "    losses.append(loss)\n",
    "    print(\"Calculated lambda_ = \", lambda_)\n",
    "plt.plot(lambdas,losses )\n",
    "pl.show()\n",
    "plt.plot(lambdas, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlzTUY2NO-rU"
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = path + 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, _ = load_csv_data(DATA_TRAIN_PATH)\n",
    "tX = replace_missing_values(tX)\n",
    "tX = normalize_data(tX)\n",
    "tX,y,_ = remove_outliers(tX, y, threshold=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11619,
     "status": "error",
     "timestamp": 1571572646681,
     "user": {
      "displayName": "JEANS EQUALS ART",
      "photoUrl": "",
      "userId": "06790227118371604310"
     },
     "user_tz": -120
    },
    "id": "D8sprKfDO-rc",
    "outputId": "edaf7fd4-23dd-4f73-a23c-7cfd48ed2920"
   },
   "outputs": [],
   "source": [
    "N,d = tX.shape\n",
    "w0 = 0.001*np.ones((d,1))\n",
    "\n",
    "#weights, losses = ridge_regression(y, tX, lambda_ = 1e-15) # param seven is chosen as the best from np.logspace(-15, -5, 10)\n",
    "#weights,loss = least_squares_GD(y, tX, w0, max_iters = 1000, gamma = 0.7)\n",
    "#weights,loss = least_squares(y,tX)\n",
    "#weights,loss = least_squares_SGD(y, tX, w0, max_iters = N, gamma = 0.7)\n",
    "weights, loss = logistic_regression(y, tX, initial_w = w0, max_iters = 1.1 * N, gamma = 0.7)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y, tX, w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHJkONYBO-rg"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = path + 'test.csv' # TODO: download train data and supply path here \n",
    "y_test, tX_test,_ = load_csv_data(DATA_TEST_PATH, zero_one = False)\n",
    "tX_test = replace_missing_values(tX_test)\n",
    "tX_test = normalize_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights, tX_test, thh = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsnwLcCJO-ri"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = path + 'log_reg.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDr45d5GA1Np"
   },
   "outputs": [],
   "source": [
    "print(\"ridge_loss = \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15883,
     "status": "ok",
     "timestamp": 1571489330763,
     "user": {
      "displayName": "JEANS EQUALS ART",
      "photoUrl": "",
      "userId": "06790227118371604310"
     },
     "user_tz": -120
    },
    "id": "A5TCPi-kAccv",
    "outputId": "6579a98c-1958-44c1-8e9e-9a8ed2dcc7d9"
   },
   "outputs": [],
   "source": [
    "print(\"SGD_loss = \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3957,
     "status": "ok",
     "timestamp": 1571490775765,
     "user": {
      "displayName": "JEANS EQUALS ART",
      "photoUrl": "",
      "userId": "06790227118371604310"
     },
     "user_tz": -120
    },
    "id": "8M_4OQgN_GNt",
    "outputId": "e3414b9f-ef63-439b-fe59-d498d6978d55"
   },
   "outputs": [],
   "source": [
    "print(\"Least_squares_loss = \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDjQovrnH98B"
   },
   "source": [
    "### **Ridge regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ3kjCrAO-re"
   },
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21721,
     "status": "ok",
     "timestamp": 1571490811177,
     "user": {
      "displayName": "JEANS EQUALS ART",
      "photoUrl": "",
      "userId": "06790227118371604310"
     },
     "user_tz": -120
    },
    "id": "bntpu0ahHT0M",
    "outputId": "13921cb8-4f3a-4d97-9c95-2af2ff4c1ab3"
   },
   "outputs": [],
   "source": [
    "acc = 0.\n",
    "for i in range(len(y_pred)):\n",
    "  if y_pred[i] == y_test[i]:\n",
    "    acc += 1\n",
    "print(acc / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDAwT3N-SoCC"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/video_games_sales.csv')\n",
    "df.info()\n",
    "cols = ['Global_Sales', 'Critic_Score', 'Critic_Count', 'User_Score', 'User_Count']\n",
    "sns_plot = sns.pairplot(df[cols])\n",
    "sns_plot.savefig('pairplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46618,
     "status": "ok",
     "timestamp": 1571153588094,
     "user": {
      "displayName": "JEANS EQUALS ART",
      "photoUrl": "",
      "userId": "06790227118371604310"
     },
     "user_tz": -120
    },
    "id": "CTAMsXkhH8pk",
    "outputId": "d74259fd-cb77-43a7-c66f-31e9b5a8655f"
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    if degree == 1:\n",
    "      return x\n",
    "    N,d = x.shape\n",
    "    ext_x = x\n",
    "    \n",
    "    for j in range(2, degree + 1):\n",
    "         ext_x = np.concatenate((ext_x, x[:,1:]**j), axis = 1)\n",
    "    return ext_x\n",
    "  \n",
    "def plot_train_test(train_errors, test_errors, lambdas):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"ridge_regression\")\n",
    "\"\"\"ridge regression demo.\"\"\"\n",
    "lambdas = np.logspace(-15, -5, 10)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "w = []\n",
    "ratio = len(y)/len(y_test)\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    weights, mse = ridge_regression(y, tX, lambda_)\n",
    "    w.append(weights)\n",
    "    rmse_tr.append(np.sqrt(mse*2.))\n",
    "    rmse_te.append(loss_function(y_test,preprocess_data(tX_test),weights))\n",
    "    print(\"proportion={p}, lambda={l}, Training MSE={tr:.3f}, Testing MSE={te:.3f}\".format(\n",
    "           p=ratio, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "\n",
    "# Plot the obtained results\n",
    "plot_train_test(rmse_tr, rmse_te, lambdas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15457,
     "status": "ok",
     "timestamp": 1571153222733,
     "user": {
      "displayName": "JEANS EQUALS ART",
      "photoUrl": "",
      "userId": "06790227118371604310"
     },
     "user_tz": -120
    },
    "id": "qZ5mk5PZpzXm",
    "outputId": "2f28d66b-e18d-4a89-bd2a-6b4c8086c69e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(preprocess_data(tX)[:,1:])\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AiGqKwW-yf1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
